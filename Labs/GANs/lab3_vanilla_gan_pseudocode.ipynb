{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Implement GAN from Pseudocode (Goodfellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup / hyperparameters ---\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "num_steps = 100000\n",
    "lr_G = 0.0002\n",
    "lr_D = 0.0002\n",
    "beta1 = 0.5        # optimizer betas for Adam\n",
    "n_critic = 1       # number of D updates per G update (often >1 for WGAN)\n",
    "use_non_saturating = True   # if True use -log(D(G)) for G (better gradients)\n",
    "use_wgan_gp = False         # set True to use WGAN-GP variant\n",
    "gp_lambda = 10.0            # gradient penalty weight for WGAN-GP\n",
    "device = \"cuda\"\n",
    "\n",
    "# Initialize models and optimizers (pseudocode)\n",
    "G = Generator(latent_dim).to(device)\n",
    "D = Discriminator().to(device)\n",
    "opt_G = Adam(G.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "opt_D = Adam(D.parameters(), lr=lr_D, betas=(beta1, 0.999))\n",
    "\n",
    "# Utility functions (pseudocode)\n",
    "def sample_noise(batch_size, latent_dim):\n",
    "    return random_normal(batch_size, latent_dim)   # e.g. N(0,1)\n",
    "\n",
    "def sample_real_batch(dataset, batch_size):\n",
    "    return dataset.next_batch(batch_size)\n",
    "\n",
    "def detach(tensor):\n",
    "    # means: no gradient flows back\n",
    "    return stop_gradient(tensor)\n",
    "\n",
    "# --- Training loop ---\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # ---- Update Discriminator ----\n",
    "    for _ in range(n_critic):\n",
    "        real_x = sample_real_batch(dataset, batch_size).to(device)\n",
    "        z = sample_noise(batch_size, latent_dim).to(device)\n",
    "        fake_x = G(z)                         # generated samples\n",
    "\n",
    "        # Discriminator outputs (probabilities or scores)\n",
    "        D_real = D(real_x)                    # shape: (batch,)\n",
    "        D_fake = D(detach(fake_x))            # detach so D step doesn't update G\n",
    "\n",
    "        if use_wgan_gp:\n",
    "            # WGAN-GP: D outputs real-valued scores (no sigmoid)\n",
    "            loss_D = mean(D_fake) - mean(D_real)   # Wasserstein critic loss\n",
    "\n",
    "            # compute gradient penalty\n",
    "            eps = random_uniform(0,1, size=batch_size).reshape(-1,1,...)\n",
    "            x_hat = eps * real_x + (1-eps) * fake_x\n",
    "            D_x_hat = D(x_hat)\n",
    "            grad = grad_of(D_x_hat, x_hat)     # gradient of D(x_hat) wrt x_hat\n",
    "            grad_norm = sqrt(sum_over_dims(grad**2) + 1e-12)\n",
    "            gp = gp_lambda * mean((grad_norm - 1.0)**2)\n",
    "            loss_D = loss_D + gp\n",
    "\n",
    "        else:\n",
    "            # Vanilla GAN: D outputs probabilities (after sigmoid)\n",
    "            # Standard minimax discriminator loss:\n",
    "            # loss_D = - mean(log(D_real)) - mean(log(1 - D_fake))\n",
    "            # In practice use stable numerics / BCE loss function\n",
    "            loss_D_real = binary_cross_entropy(D_real, ones_like(D_real))\n",
    "            loss_D_fake = binary_cross_entropy(D_fake, zeros_like(D_fake))\n",
    "            loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "        # Backprop and optimizer step for D\n",
    "        opt_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "    # ---- Update Generator ----\n",
    "    z = sample_noise(batch_size, latent_dim).to(device)\n",
    "    fake_x = G(z)\n",
    "    D_fake_for_G = D(fake_x)    # do NOT detach: gradients should flow to G\n",
    "\n",
    "    if use_wgan_gp:\n",
    "        # WGAN generator objective: minimize -E[D(G(z))]\n",
    "        loss_G = - mean(D_fake_for_G)\n",
    "\n",
    "    else:\n",
    "        if use_non_saturating:\n",
    "            # Non-saturating heuristic:\n",
    "            # loss_G = - mean(log(D(G(z)))) which gives stronger gradients early\n",
    "            loss_G = binary_cross_entropy(D_fake_for_G, ones_like(D_fake_for_G))\n",
    "        else:\n",
    "            # Minimax (saturating) version (less commonly used in practice):\n",
    "            # loss_G = mean(log(1 - D(G(z))))\n",
    "            loss_G = binary_cross_entropy(D_fake_for_G, zeros_like(D_fake_for_G))  # conceptual\n",
    "\n",
    "    # Backprop and optimizer step for G\n",
    "    opt_G.zero_grad()\n",
    "    loss_G.backward()\n",
    "    opt_G.step()\n",
    "\n",
    "    # --- Logging & periodic evaluation ---\n",
    "    if step % log_interval == 0:\n",
    "        print(\"step\", step, \"loss_D\", loss_D.item(), \"loss_G\", loss_G.item())\n",
    "    if step % sample_interval == 0:\n",
    "        z_vis = sample_noise(visual_batch, latent_dim)\n",
    "        imgs = G(z_vis)                     # generate images for monitoring\n",
    "        save_images(imgs, f\"samples/step_{step}.png\")\n",
    "    if step % checkpoint_interval == 0:\n",
    "        save_model(G, f\"G_{step}.pt\")\n",
    "        save_model(D, f\"D_{step}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
